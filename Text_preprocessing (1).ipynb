{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23a4eb83-5c11-4692-9b48-d7944beddc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8044814-1ac3-479d-be60-4527f4493530",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b104dc9-dd2d-44dd-af80-8ffc64c0545c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d6fc05e-e973-427e-a7f1-004b09d67530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. the plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). while some may be disappointed when they realize this is not match point 2: risk addiction, i thought it was proof that woody allen is still fully in control of the style many of us have grown to love.<br /><br />this was the most i\\'d laughed at one of woody\\'s comedies in years (dare i say a decade?). while i\\'ve never been impressed with scarlet johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />this may not be the crown jewel of his career, but it was wittier than \"devil wears prada\" and more interesting than \"superman\" a great comedy to go see with friends.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"review\"][2].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c2645c1-2a0a-427d-9728-6d68089ef114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one of the other reviewers has mentioned that ...\n",
       "1        a wonderful little production. <br /><br />the...\n",
       "2        i thought this was a wonderful way to spend ti...\n",
       "3        basically there's a family where a little boy ...\n",
       "4        petter mattei's \"love in the time of money\" is...\n",
       "                               ...                        \n",
       "49995    i thought this movie did a down right good job...\n",
       "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    i am a catholic taught in parochial elementary...\n",
       "49998    i'm going to have to disagree with the previou...\n",
       "49999    no one expects the star trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"review\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa2fff31-7d83-4085-9301-e1dc1cbcfda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_html_tags(txt):\n",
    "    pattern = re.compile(\"<.*?>\")\n",
    "    return pattern.sub(r\" \",txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0116bfd9-3de4-4782-87c7-c4a099cc66f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is AI generated \n",
    "# import re\n",
    "\n",
    "# def remove_html_tags(data):\n",
    "#     # Using regex to match HTML tags\n",
    "#     cleanr = re.compile('<.*?>')\n",
    "#     cleantext = re.sub(cleanr, '', data)\n",
    "#     return cleantext\n",
    "\n",
    "# # Example usage\n",
    "# html_data = '<p>Hello, <b>world!</b></p>'\n",
    "# clean_data = remove_html_tags(html_data)\n",
    "# print(clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840c9d70-9404-404b-8937-0a11e871963c",
   "metadata": {},
   "source": [
    "Substitution: re.sub(cleanr, '', data) vs. pattern.sub(r\" \", txt)\n",
    "\n",
    "The first function(Ai generated) removes tags completely.\n",
    "\n",
    "The second function replaces tags with a space.\n",
    "\n",
    "Example:\n",
    "\n",
    "For input '<p>Hello, <b>world!</b></p>':\n",
    "\n",
    "Function 1 Output: 'Hello, world!'\n",
    "\n",
    "Function 2 Output: 'Hello, world! ' (with extra spaces where tags were)\n",
    "\n",
    "Both functions achieve the goal of removing HTML tags, but the choice between them depends on whether you want spaces in place of the removed tags or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17da706d-092a-4f82-a39d-c49af7e74905",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"review\"]=df[\"review\"].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae2c78a7-83b2-48ad-be9e-924da18edcbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wonderful little production.   The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece.   The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life.   The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"review\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50782b12-a6bb-4f55-bec1-05be6bb52c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out  and also visit  for more info.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_urls(data):\n",
    "    # Using regex to match URLs\n",
    "    url_pattern = re.compile(r'http[s]?://\\S+|www\\.\\S+')\n",
    "    clean_data = re.sub(url_pattern, '', data)\n",
    "    return clean_data\n",
    "\n",
    "# Example usage\n",
    "text_with_urls = 'Check out https://www.example.com and also visit http://example.org for more info.'\n",
    "clean_text = remove_urls(text_with_urls)\n",
    "print(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb55970b-a9d2-472b-97b5-61654d3e1800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "798aac9f-fdd8-4a00-90c4-e5cbb602622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = string.punctuation\n",
    "def remove_punctuations(txt):\n",
    "    return txt.translate(str.maketrans(\"\",\"\",exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f20de24-6b4f-4d28-bfcd-efd7653f4d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        One of the other reviewers has mentioned that ...\n",
       "1        A wonderful little production   The filming te...\n",
       "2        I thought this was a wonderful way to spend ti...\n",
       "3        Basically theres a family where a little boy J...\n",
       "4        Petter Matteis Love in the Time of Money is a ...\n",
       "                               ...                        \n",
       "49995    I thought this movie did a down right good job...\n",
       "49996    Bad plot bad dialogue bad acting idiotic direc...\n",
       "49997    I am a Catholic taught in parochial elementary...\n",
       "49998    Im going to have to disagree with the previous...\n",
       "49999    No one expects the Star Trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"review\"].apply(remove_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5d996e0-8639-46ea-a696-bcb5c7894a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chat word removal "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76aa4e4-f085-444c-947c-0d7eea7a70b1",
   "metadata": {},
   "source": [
    "# Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1b5b7dd-8d6c-4e58-ab53-802b7ae798d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e8d21af2-c7f2-4ef5-b2b8-bcafde21df03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7b252e74-ac7d-4b2f-a768-e202da331bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aber',\n",
       " 'alle',\n",
       " 'allem',\n",
       " 'allen',\n",
       " 'aller',\n",
       " 'alles',\n",
       " 'als',\n",
       " 'also',\n",
       " 'am',\n",
       " 'an',\n",
       " 'ander',\n",
       " 'andere',\n",
       " 'anderem',\n",
       " 'anderen',\n",
       " 'anderer',\n",
       " 'anderes',\n",
       " 'anderm',\n",
       " 'andern',\n",
       " 'anderr',\n",
       " 'anders',\n",
       " 'auch',\n",
       " 'auf',\n",
       " 'aus',\n",
       " 'bei',\n",
       " 'bin',\n",
       " 'bis',\n",
       " 'bist',\n",
       " 'da',\n",
       " 'damit',\n",
       " 'dann',\n",
       " 'der',\n",
       " 'den',\n",
       " 'des',\n",
       " 'dem',\n",
       " 'die',\n",
       " 'das',\n",
       " 'dass',\n",
       " 'da√ü',\n",
       " 'derselbe',\n",
       " 'derselben',\n",
       " 'denselben',\n",
       " 'desselben',\n",
       " 'demselben',\n",
       " 'dieselbe',\n",
       " 'dieselben',\n",
       " 'dasselbe',\n",
       " 'dazu',\n",
       " 'dein',\n",
       " 'deine',\n",
       " 'deinem',\n",
       " 'deinen',\n",
       " 'deiner',\n",
       " 'deines',\n",
       " 'denn',\n",
       " 'derer',\n",
       " 'dessen',\n",
       " 'dich',\n",
       " 'dir',\n",
       " 'du',\n",
       " 'dies',\n",
       " 'diese',\n",
       " 'diesem',\n",
       " 'diesen',\n",
       " 'dieser',\n",
       " 'dieses',\n",
       " 'doch',\n",
       " 'dort',\n",
       " 'durch',\n",
       " 'ein',\n",
       " 'eine',\n",
       " 'einem',\n",
       " 'einen',\n",
       " 'einer',\n",
       " 'eines',\n",
       " 'einig',\n",
       " 'einige',\n",
       " 'einigem',\n",
       " 'einigen',\n",
       " 'einiger',\n",
       " 'einiges',\n",
       " 'einmal',\n",
       " 'er',\n",
       " 'ihn',\n",
       " 'ihm',\n",
       " 'es',\n",
       " 'etwas',\n",
       " 'euer',\n",
       " 'eure',\n",
       " 'eurem',\n",
       " 'euren',\n",
       " 'eurer',\n",
       " 'eures',\n",
       " 'f√ºr',\n",
       " 'gegen',\n",
       " 'gewesen',\n",
       " 'hab',\n",
       " 'habe',\n",
       " 'haben',\n",
       " 'hat',\n",
       " 'hatte',\n",
       " 'hatten',\n",
       " 'hier',\n",
       " 'hin',\n",
       " 'hinter',\n",
       " 'ich',\n",
       " 'mich',\n",
       " 'mir',\n",
       " 'ihr',\n",
       " 'ihre',\n",
       " 'ihrem',\n",
       " 'ihren',\n",
       " 'ihrer',\n",
       " 'ihres',\n",
       " 'euch',\n",
       " 'im',\n",
       " 'in',\n",
       " 'indem',\n",
       " 'ins',\n",
       " 'ist',\n",
       " 'jede',\n",
       " 'jedem',\n",
       " 'jeden',\n",
       " 'jeder',\n",
       " 'jedes',\n",
       " 'jene',\n",
       " 'jenem',\n",
       " 'jenen',\n",
       " 'jener',\n",
       " 'jenes',\n",
       " 'jetzt',\n",
       " 'kann',\n",
       " 'kein',\n",
       " 'keine',\n",
       " 'keinem',\n",
       " 'keinen',\n",
       " 'keiner',\n",
       " 'keines',\n",
       " 'k√∂nnen',\n",
       " 'k√∂nnte',\n",
       " 'machen',\n",
       " 'man',\n",
       " 'manche',\n",
       " 'manchem',\n",
       " 'manchen',\n",
       " 'mancher',\n",
       " 'manches',\n",
       " 'mein',\n",
       " 'meine',\n",
       " 'meinem',\n",
       " 'meinen',\n",
       " 'meiner',\n",
       " 'meines',\n",
       " 'mit',\n",
       " 'muss',\n",
       " 'musste',\n",
       " 'nach',\n",
       " 'nicht',\n",
       " 'nichts',\n",
       " 'noch',\n",
       " 'nun',\n",
       " 'nur',\n",
       " 'ob',\n",
       " 'oder',\n",
       " 'ohne',\n",
       " 'sehr',\n",
       " 'sein',\n",
       " 'seine',\n",
       " 'seinem',\n",
       " 'seinen',\n",
       " 'seiner',\n",
       " 'seines',\n",
       " 'selbst',\n",
       " 'sich',\n",
       " 'sie',\n",
       " 'ihnen',\n",
       " 'sind',\n",
       " 'so',\n",
       " 'solche',\n",
       " 'solchem',\n",
       " 'solchen',\n",
       " 'solcher',\n",
       " 'solches',\n",
       " 'soll',\n",
       " 'sollte',\n",
       " 'sondern',\n",
       " 'sonst',\n",
       " '√ºber',\n",
       " 'um',\n",
       " 'und',\n",
       " 'uns',\n",
       " 'unsere',\n",
       " 'unserem',\n",
       " 'unseren',\n",
       " 'unser',\n",
       " 'unseres',\n",
       " 'unter',\n",
       " 'viel',\n",
       " 'vom',\n",
       " 'von',\n",
       " 'vor',\n",
       " 'w√§hrend',\n",
       " 'war',\n",
       " 'waren',\n",
       " 'warst',\n",
       " 'was',\n",
       " 'weg',\n",
       " 'weil',\n",
       " 'weiter',\n",
       " 'welche',\n",
       " 'welchem',\n",
       " 'welchen',\n",
       " 'welcher',\n",
       " 'welches',\n",
       " 'wenn',\n",
       " 'werde',\n",
       " 'werden',\n",
       " 'wie',\n",
       " 'wieder',\n",
       " 'will',\n",
       " 'wir',\n",
       " 'wird',\n",
       " 'wirst',\n",
       " 'wo',\n",
       " 'wollen',\n",
       " 'wollte',\n",
       " 'w√ºrde',\n",
       " 'w√ºrden',\n",
       " 'zu',\n",
       " 'zum',\n",
       " 'zur',\n",
       " 'zwar',\n",
       " 'zwischen']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"german\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ca80e99-2018-4da4-a96e-3fe072a849d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'm√™me',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " '√†',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " '√©t√©',\n",
       " '√©t√©e',\n",
       " '√©t√©es',\n",
       " '√©t√©s',\n",
       " '√©tant',\n",
       " '√©tante',\n",
       " '√©tants',\n",
       " '√©tantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " '√™tes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " '√©tais',\n",
       " '√©tait',\n",
       " '√©tions',\n",
       " '√©tiez',\n",
       " '√©taient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'f√ªmes',\n",
       " 'f√ªtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'f√ªt',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'e√ªmes',\n",
       " 'e√ªtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'e√ªt',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"french\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e0d0c4a2-f421-4644-8d58-94853f09c9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text= []\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words(\"english\"):\n",
    "            new_text.append(\"\")\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return \"\".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c47daf55-d304-4f0e-a8a1-1799d6370175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production.   The filming t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production.   The filming t...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a734751a-af85-45e0-89c3-90808d0d2e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This example sentence .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define a function to remove stop words\n",
    "def remove_stop_words(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Test the function\n",
    "text = \"This is an example sentence.\"\n",
    "print(remove_stop_words(text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "911e3d55-1c09-45ed-a3e6-7702bd49ae5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        One reviewers mentioned watching 1 Oz episode ...\n",
       "1        A wonderful little production . The filming te...\n",
       "2        I thought wonderful way spend time hot summer ...\n",
       "3        Basically 's family little boy ( Jake ) thinks...\n",
       "4        Petter Mattei 's `` Love Time Money '' visuall...\n",
       "                               ...                        \n",
       "49995    I thought movie right good job . It n't creati...\n",
       "49996    Bad plot , bad dialogue , bad acting , idiotic...\n",
       "49997    I Catholic taught parochial elementary schools...\n",
       "49998    I 'm going disagree previous comment side Malt...\n",
       "49999    No one expects Star Trek movies high art , fan...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"review\"].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9525f50b-2bea-4c02-bbf9-74b1ac4a8d9f",
   "metadata": {},
   "source": [
    "# Handling emojis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c0ae266-fdae-4f80-9f68-7197914fd582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can either remove or replace\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613d1f87-850a-495f-825b-2eda9f132dbf",
   "metadata": {},
   "source": [
    "### we can remove emiji in two ways :- \n",
    "1. BY regular expression which is not generally prefrred\n",
    "2. By \" replace_emoji \" in emoji library  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd6f7f10-1fd1-4f17-99c0-a43f1c105c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello ! This is an example \n"
     ]
    }
   ],
   "source": [
    "# By Regular expression\n",
    "import re\n",
    "\n",
    "def remove_emojis_regex(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
    "        u\"\\U0001F700-\\U0001F77F\"  # Alchemical symbols\n",
    "        u\"\\U0001F780-\\U0001F7FF\"  # Geometric symbols\n",
    "        u\"\\U0001F800-\\U0001F8FF\"  # Supplemental symbols\n",
    "        u\"\\U0001F900-\\U0001F9FF\"  # Various emojis\n",
    "        u\"\\U0001FA00-\\U0001FA6F\"  # Symbols and Pictographs Extended-A\n",
    "        u\"\\U0001FA70-\\U0001FAFF\"  # More emojis\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# Test case\n",
    "text = \"Hello üòä! This is an example üöÄ\"\n",
    "print(remove_emojis_regex(text))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "30202705-4976-4cca-973f-cb2adf70070e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello ! This is an example \n"
     ]
    }
   ],
   "source": [
    "# by function \n",
    "import emoji\n",
    "\n",
    "def remove_emojis(text):\n",
    "    return emoji.replace_emoji(text, replace='')  # Removes emojis\n",
    "\n",
    "# Test the function\n",
    "text = \"Hello üòä! This is an example üöÄ\"\n",
    "print(remove_emojis(text))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01553a5a-d7de-4850-89f9-f14d51cc87e7",
   "metadata": {},
   "source": [
    "###  Replacing emoji with their meaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d2933dde-2f8a-4e43-b2cd-0a85dc6249e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello :smiling_face_with_smiling_eyes:! This is an example :rocket:\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "\n",
    "def replace_emojis_with_text(text):\n",
    "    return emoji.demojize(text)  # Converts emojis to text\n",
    "\n",
    "# Test case\n",
    "text = \"Hello üòä! This is an example üöÄ\"\n",
    "print(replace_emojis_with_text(text))\n",
    "# Output: \"Hello :blush:! This is an example :rocket:\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1fcb4ebf-66a4-4c4d-8de7-27cebf0f89c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is awesome! üêç\n"
     ]
    }
   ],
   "source": [
    "# creating emoji with text \n",
    "# emojize function\n",
    "print(emoji.emojize(\"Python is awesome! :snake:\"))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fa0d79-187c-4ae3-9c39-1207a99c43b9",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "64b36401-5c99-4ee1-96cb-be60ff565131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "70cbbcce-f8d4-437b-a64c-cb055ea773a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The rapid advancement of artificial intelligence (AI) has transformed numerous industries, ranging from healthcare to finance and entertainment. In healthcare, AI-powered diagnostic tools help doctors detect diseases like cancer at an early stage, significantly improving survival rates. Similarly, in finance, AI algorithms analyze market trends and predict stock movements, allowing investors to make informed decisions. Meanwhile, in the entertainment industry, AI-driven recommendation systems personalize user experiences on streaming platforms like Netflix and Spotify. However, despite these benefits, AI also raises ethical concerns, including data privacy issues, job displacement, and algorithmic biases. Companies must implement responsible AI practices to ensure fairness and transparency. As technology continues to evolve, the balance between innovation and ethical considerations will remain a critical challenge for policymakers and industry leaders worldwide.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6bf26b24-bc2b-4393-babb-914130cef2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'rapid',\n",
       " 'advancement',\n",
       " 'of',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " '(',\n",
       " 'AI',\n",
       " ')',\n",
       " 'has',\n",
       " 'transformed',\n",
       " 'numerous',\n",
       " 'industries',\n",
       " ',',\n",
       " 'ranging',\n",
       " 'from',\n",
       " 'healthcare',\n",
       " 'to',\n",
       " 'finance',\n",
       " 'and',\n",
       " 'entertainment',\n",
       " '.',\n",
       " 'In',\n",
       " 'healthcare',\n",
       " ',',\n",
       " 'AI-powered',\n",
       " 'diagnostic',\n",
       " 'tools',\n",
       " 'help',\n",
       " 'doctors',\n",
       " 'detect',\n",
       " 'diseases',\n",
       " 'like',\n",
       " 'cancer',\n",
       " 'at',\n",
       " 'an',\n",
       " 'early',\n",
       " 'stage',\n",
       " ',',\n",
       " 'significantly',\n",
       " 'improving',\n",
       " 'survival',\n",
       " 'rates',\n",
       " '.',\n",
       " 'Similarly',\n",
       " ',',\n",
       " 'in',\n",
       " 'finance',\n",
       " ',',\n",
       " 'AI',\n",
       " 'algorithms',\n",
       " 'analyze',\n",
       " 'market',\n",
       " 'trends',\n",
       " 'and',\n",
       " 'predict',\n",
       " 'stock',\n",
       " 'movements',\n",
       " ',',\n",
       " 'allowing',\n",
       " 'investors',\n",
       " 'to',\n",
       " 'make',\n",
       " 'informed',\n",
       " 'decisions',\n",
       " '.',\n",
       " 'Meanwhile',\n",
       " ',',\n",
       " 'in',\n",
       " 'the',\n",
       " 'entertainment',\n",
       " 'industry',\n",
       " ',',\n",
       " 'AI-driven',\n",
       " 'recommendation',\n",
       " 'systems',\n",
       " 'personalize',\n",
       " 'user',\n",
       " 'experiences',\n",
       " 'on',\n",
       " 'streaming',\n",
       " 'platforms',\n",
       " 'like',\n",
       " 'Netflix',\n",
       " 'and',\n",
       " 'Spotify',\n",
       " '.',\n",
       " 'However',\n",
       " ',',\n",
       " 'despite',\n",
       " 'these',\n",
       " 'benefits',\n",
       " ',',\n",
       " 'AI',\n",
       " 'also',\n",
       " 'raises',\n",
       " 'ethical',\n",
       " 'concerns',\n",
       " ',',\n",
       " 'including',\n",
       " 'data',\n",
       " 'privacy',\n",
       " 'issues',\n",
       " ',',\n",
       " 'job',\n",
       " 'displacement',\n",
       " ',',\n",
       " 'and',\n",
       " 'algorithmic',\n",
       " 'biases',\n",
       " '.',\n",
       " 'Companies',\n",
       " 'must',\n",
       " 'implement',\n",
       " 'responsible',\n",
       " 'AI',\n",
       " 'practices',\n",
       " 'to',\n",
       " 'ensure',\n",
       " 'fairness',\n",
       " 'and',\n",
       " 'transparency',\n",
       " '.',\n",
       " 'As',\n",
       " 'technology',\n",
       " 'continues',\n",
       " 'to',\n",
       " 'evolve',\n",
       " ',',\n",
       " 'the',\n",
       " 'balance',\n",
       " 'between',\n",
       " 'innovation',\n",
       " 'and',\n",
       " 'ethical',\n",
       " 'considerations',\n",
       " 'will',\n",
       " 'remain',\n",
       " 'a',\n",
       " 'critical',\n",
       " 'challenge',\n",
       " 'for',\n",
       " 'policymakers',\n",
       " 'and',\n",
       " 'industry',\n",
       " 'leaders',\n",
       " 'worldwide',\n",
       " '.']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5785fc9c-912a-4579-8256-6e685347b9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The rapid advancement of artificial intelligence (AI) has transformed numerous industries, ranging from healthcare to finance and entertainment.',\n",
       " 'In healthcare, AI-powered diagnostic tools help doctors detect diseases like cancer at an early stage, significantly improving survival rates.',\n",
       " 'Similarly, in finance, AI algorithms analyze market trends and predict stock movements, allowing investors to make informed decisions.',\n",
       " 'Meanwhile, in the entertainment industry, AI-driven recommendation systems personalize user experiences on streaming platforms like Netflix and Spotify.',\n",
       " 'However, despite these benefits, AI also raises ethical concerns, including data privacy issues, job displacement, and algorithmic biases.',\n",
       " 'Companies must implement responsible AI practices to ensure fairness and transparency.',\n",
       " 'As technology continues to evolve, the balance between innovation and ethical considerations will remain a critical challenge for policymakers and industry leaders worldwide.']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5210856-0b43-4a1b-9473-6eaa1c5a8c7b",
   "metadata": {},
   "source": [
    "### Spacy tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "462d9f3a-602e-46ef-ba82-5400fa1d7214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy is working!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"SpaCy is working!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c588ba4a-9165-4d02-8dbe-a5102233f589",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a4431c1a-1b3d-4bad-8134-575f4ae27ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\n",
      "rapid\n",
      "advancement\n",
      "of\n",
      "artificial\n",
      "intelligence\n",
      "(\n",
      "AI\n",
      ")\n",
      "has\n",
      "transformed\n",
      "numerous\n",
      "industries\n",
      ",\n",
      "ranging\n",
      "from\n",
      "healthcare\n",
      "to\n",
      "finance\n",
      "and\n",
      "entertainment\n",
      ".\n",
      "In\n",
      "healthcare\n",
      ",\n",
      "AI\n",
      "-\n",
      "powered\n",
      "diagnostic\n",
      "tools\n",
      "help\n",
      "doctors\n",
      "detect\n",
      "diseases\n",
      "like\n",
      "cancer\n",
      "at\n",
      "an\n",
      "early\n",
      "stage\n",
      ",\n",
      "significantly\n",
      "improving\n",
      "survival\n",
      "rates\n",
      ".\n",
      "Similarly\n",
      ",\n",
      "in\n",
      "finance\n",
      ",\n",
      "AI\n",
      "algorithms\n",
      "analyze\n",
      "market\n",
      "trends\n",
      "and\n",
      "predict\n",
      "stock\n",
      "movements\n",
      ",\n",
      "allowing\n",
      "investors\n",
      "to\n",
      "make\n",
      "informed\n",
      "decisions\n",
      ".\n",
      "Meanwhile\n",
      ",\n",
      "in\n",
      "the\n",
      "entertainment\n",
      "industry\n",
      ",\n",
      "AI\n",
      "-\n",
      "driven\n",
      "recommendation\n",
      "systems\n",
      "personalize\n",
      "user\n",
      "experiences\n",
      "on\n",
      "streaming\n",
      "platforms\n",
      "like\n",
      "Netflix\n",
      "and\n",
      "Spotify\n",
      ".\n",
      "However\n",
      ",\n",
      "despite\n",
      "these\n",
      "benefits\n",
      ",\n",
      "AI\n",
      "also\n",
      "raises\n",
      "ethical\n",
      "concerns\n",
      ",\n",
      "including\n",
      "data\n",
      "privacy\n",
      "issues\n",
      ",\n",
      "job\n",
      "displacement\n",
      ",\n",
      "and\n",
      "algorithmic\n",
      "biases\n",
      ".\n",
      "Companies\n",
      "must\n",
      "implement\n",
      "responsible\n",
      "AI\n",
      "practices\n",
      "to\n",
      "ensure\n",
      "fairness\n",
      "and\n",
      "transparency\n",
      ".\n",
      "As\n",
      "technology\n",
      "continues\n",
      "to\n",
      "evolve\n",
      ",\n",
      "the\n",
      "balance\n",
      "between\n",
      "innovation\n",
      "and\n",
      "ethical\n",
      "considerations\n",
      "will\n",
      "remain\n",
      "a\n",
      "critical\n",
      "challenge\n",
      "for\n",
      "policymakers\n",
      "and\n",
      "industry\n",
      "leaders\n",
      "worldwide\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc :\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c19c7837-bb9f-4944-8cc1-5157809aefc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'rapid', 'advancement', 'of', 'artificial', 'intelligence', '(', 'AI', ')', 'has', 'transformed', 'numerous', 'industries', ',', 'ranging', 'from', 'healthcare', 'to', 'finance', 'and', 'entertainment', '.', 'In', 'healthcare', ',', 'AI', '-', 'powered', 'diagnostic', 'tools', 'help', 'doctors', 'detect', 'diseases', 'like', 'cancer', 'at', 'an', 'early', 'stage', ',', 'significantly', 'improving', 'survival', 'rates', '.', 'Similarly', ',', 'in', 'finance', ',', 'AI', 'algorithms', 'analyze', 'market', 'trends', 'and', 'predict', 'stock', 'movements', ',', 'allowing', 'investors', 'to', 'make', 'informed', 'decisions', '.', 'Meanwhile', ',', 'in', 'the', 'entertainment', 'industry', ',', 'AI', '-', 'driven', 'recommendation', 'systems', 'personalize', 'user', 'experiences', 'on', 'streaming', 'platforms', 'like', 'Netflix', 'and', 'Spotify', '.', 'However', ',', 'despite', 'these', 'benefits', ',', 'AI', 'also', 'raises', 'ethical', 'concerns', ',', 'including', 'data', 'privacy', 'issues', ',', 'job', 'displacement', ',', 'and', 'algorithmic', 'biases', '.', 'Companies', 'must', 'implement', 'responsible', 'AI', 'practices', 'to', 'ensure', 'fairness', 'and', 'transparency', '.', 'As', 'technology', 'continues', 'to', 'evolve', ',', 'the', 'balance', 'between', 'innovation', 'and', 'ethical', 'considerations', 'will', 'remain', 'a', 'critical', 'challenge', 'for', 'policymakers', 'and', 'industry', 'leaders', 'worldwide', '.']\n"
     ]
    }
   ],
   "source": [
    "#or we can save them in a list \n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fe45f2-b5cc-463b-be85-0ca10ae3f40e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97de2688-6407-4f80-8066-c7d1880f7017",
   "metadata": {},
   "source": [
    "# Stemming \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c21e8168-9116-449b-962d-3d9df96b7e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0f4df089-5614-46df-a6a4-2f04ed11126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9f498b85-41a7-44d4-a7c6-8ddb3dac70ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk wlk walk walk'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"Walk wlKS waLking  walked\"\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6f88f136-5864-4c20-8839-5f15a7a8353a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it's not preachy or boring. It just never gets old, despite my having seen it some 15 or more times in the last 25 years. Paul Lukas' performance brings tears to my eyes, and Bette Davis, in one of her very few truly sympathetic roles, is a delight. The kids are, as grandma says, more like \"dressed-up midgets\" than children, but that only makes them more fun to watch. And the mother's slow awakening to what's happening in the world and under her own roof is believable and startling. If I had a dozen thumbs, they'd all be \"up\" for this movie.\n"
     ]
    }
   ],
   "source": [
    "txt = df[\"review\"][5]\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1646b99c-f39d-49df-8e3f-d8c127f49539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probabl my all-tim favorit movie, a stori of selflessness, sacrific and dedic to a nobl cause, but it\\' not preachi or boring. it just never get old, despit my have seen it some 15 or more time in the last 25 years. paul lukas\\' perform bring tear to my eyes, and bett davis, in one of her veri few truli sympathet roles, is a delight. the kid are, as grandma says, more like \"dressed-up midgets\" than children, but that onli make them more fun to watch. and the mother\\' slow awaken to what\\' happen in the world and under her own roof is believ and startling. if i had a dozen thumbs, they\\'d all be \"up\" for thi movie.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122df535-d6bc-4e48-ada6-5532a31e7dfe",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1484f28-b572-477f-aed2-b372c74f6115",
   "metadata": {},
   "source": [
    "### Using nltk "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7d3673-9361-4920-8930-44ae25e0b1c1",
   "metadata": {},
   "source": [
    "### Using SpaCy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b78a97-9dcd-4537-8e64-0334711132a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy model (English)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text\n",
    "text = \"The cats are running faster than the dogs.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Lemmatization\n",
    "for token in doc:\n",
    "    print(f\"Word: {token.text} | Lemma: {token.lemma_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9927b65b-c0da-4f5a-acbd-e8fdd4ecc298",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
